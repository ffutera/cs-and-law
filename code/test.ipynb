{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 147ms/step - loss: 0.2961 - val_loss: 0.1314\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "[[1.6452286  0.         0.9446424  1.4481316  0.6801096  0.\n",
      "  0.21253724 1.6036898  2.1334336  0.         1.1536818  1.9334687\n",
      "  1.0276539  0.         1.1209863  0.12588558 1.1993984  0.\n",
      "  1.1389915  2.1965058  0.9185519  0.         1.4154941  0.\n",
      "  0.0939149  0.98624724 1.1080189  0.62442327 0.99399525 0.\n",
      "  1.7361144  0.         1.6559418  0.         2.0530734  2.2301843\n",
      "  0.46218324 0.07035834 0.         2.827939   0.25931057 0.\n",
      "  1.1419768  3.2311444  0.4699456  0.         3.3032088  1.4368883\n",
      "  0.         0.26433992 1.3918743  2.3706942  1.113142   0.\n",
      "  3.6867752  0.         0.         1.1066552  1.4354508  1.2797693\n",
      "  1.1876783  0.         2.3804173  0.         1.1407547  0.47485378\n",
      "  1.8621093  1.6025746  0.8315718  0.         0.94038695 2.649459\n",
      "  0.5292797  0.         1.6434708  3.4190927  0.83010584 0.21012408\n",
      "  2.481738   1.408861   0.         0.         0.9343823  2.9707007\n",
      "  1.161471   0.44838893 3.1421797  0.         0.         1.0094796\n",
      "  1.0971743  0.6835475  1.1356854  0.         2.2213345  0.\n",
      "  0.536638   0.4046129  1.8402705  0.62326723 0.2730303  1.3584328\n",
      "  0.4601468  1.4287705  0.         0.         1.1212744  2.2447693\n",
      "  0.         2.567048   1.0471995  1.1850744  0.         0.7582414\n",
      "  0.22821835 1.426597   0.35912943 2.0249605  2.5864267  0.6608703\n",
      "  0.         0.85525006 0.7307984  0.         1.2559211  0.5195268\n",
      "  1.446378   0.6710917 ]\n",
      " [1.9421417  0.         0.7501341  1.5102338  1.360794   0.\n",
      "  0.2721758  1.775763   2.447538   0.         0.9545084  1.8792382\n",
      "  1.269529   0.         1.9816706  0.         1.4186965  0.15360644\n",
      "  1.0637555  1.4926801  1.0431978  0.         2.1860423  0.\n",
      "  0.5914271  0.8816658  1.4229333  0.63697517 0.7718472  0.\n",
      "  1.075607   0.         1.2150517  0.         1.5697137  2.1758075\n",
      "  0.2365598  0.         0.6422375  2.9367108  0.42691198 0.\n",
      "  1.5615717  3.2951725  0.69954234 0.         2.148111   1.7211359\n",
      "  0.         0.04189505 0.643605   2.616062   1.1708033  0.\n",
      "  3.6327286  0.         0.         1.2948507  1.3213956  0.57936186\n",
      "  1.618297   0.         1.9915575  0.         0.80082273 0.42995378\n",
      "  1.9330616  1.8942927  0.50177    0.48020095 0.46206653 2.346769\n",
      "  0.46864346 0.         1.4015026  3.4910445  0.35995117 0.7741981\n",
      "  2.0827699  1.3850211  0.         0.08969141 0.6746093  2.4517922\n",
      "  0.79158634 0.60481584 3.389444   0.         0.         0.9261807\n",
      "  1.2028896  0.56938946 1.1288105  0.1173612  1.5130092  0.27966705\n",
      "  0.37575817 0.8822988  1.7110846  0.4464146  0.3379169  1.0756686\n",
      "  0.9516306  1.3137281  0.         0.         1.0271864  2.1557412\n",
      "  0.         2.1614451  0.87514967 1.2070606  0.         0.78418005\n",
      "  0.34986386 1.3231863  0.7095107  1.4883896  3.0136802  0.\n",
      "  0.         0.79565084 0.7797674  0.04193266 0.9597631  0.140822\n",
      "  1.395539   0.71130747]\n",
      " [0.99305844 0.19669765 0.6497484  1.3117839  0.8518247  0.\n",
      "  0.51054114 1.8089757  2.1276636  0.         0.8493755  1.2799604\n",
      "  1.4440361  0.         1.3907181  0.34860417 0.8734873  0.83506924\n",
      "  0.8928065  1.0909208  0.6085753  0.         2.4002671  0.\n",
      "  0.7835507  0.9520818  1.5245833  0.35031357 0.9651251  0.\n",
      "  0.5705677  0.         0.9904492  0.40823162 1.4916236  1.8146144\n",
      "  0.57202685 0.         0.21365128 2.7019007  1.3384016  0.\n",
      "  0.54461586 2.9514225  0.16668649 0.         1.9491398  2.214294\n",
      "  0.         0.6091223  0.33857372 1.9965438  0.81612295 0.\n",
      "  3.6003115  0.         0.         1.4189353  1.6086206  0.7435148\n",
      "  1.1376779  0.         1.2571758  0.05313338 0.95768106 1.0929184\n",
      "  1.5086883  0.9812579  0.66475564 0.47289622 0.4709255  1.9072754\n",
      "  1.3065294  0.         1.0446837  3.239133   0.         0.43032283\n",
      "  1.4510609  1.9866081  0.         0.         0.18204525 2.2651732\n",
      "  1.0257858  0.26685536 3.5498834  0.         0.         1.0640622\n",
      "  1.3066084  0.4771638  1.0321226  0.         1.5717736  0.\n",
      "  0.40909794 0.83302855 1.2110893  0.37883136 0.51937056 1.0075787\n",
      "  0.99904495 0.55180395 0.01668805 0.         1.3743097  1.7890117\n",
      "  0.         1.5636318  0.80740184 1.599728   0.         0.44729647\n",
      "  0.04491821 1.804965   0.3889439  1.6960802  2.3889601  0.\n",
      "  0.         0.8255017  0.68765146 0.03860479 1.0595008  0.12017459\n",
      "  1.360446   0.3593471 ]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "XA must be a 2-dimensional array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m3\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_imgs_flatten[selected_indices])\n\u001b[0;32m--> 105\u001b[0m similar_images_indices \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_imgs_flatten\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_imgs_flatten\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m display_similar_images(x_test, selected_indices, similar_images_indices)\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mfind_similar_images\u001b[0;34m(artist_embeddings, copied_embeddings)\u001b[0m\n\u001b[1;32m     67\u001b[0m similar_images_indices \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m copied_embeddings:\n\u001b[0;32m---> 69\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martist_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     closest_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances)[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m4\u001b[39m]  \u001b[38;5;66;03m# Exclude self\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     similar_images_indices\u001b[38;5;241m.\u001b[39mappend(closest_indices)\n",
      "File \u001b[0;32m~/Desktop/cs-and-law/venv/lib/python3.10/site-packages/scipy/spatial/distance.py:3104\u001b[0m, in \u001b[0;36mcdist\u001b[0;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[1;32m   3101\u001b[0m sB \u001b[38;5;241m=\u001b[39m XB\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   3103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 3104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXA must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sB) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   3106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXB must be a 2-dimensional array.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: XA must be a 2-dimensional array."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the MNIST dataset.\"\"\"\n",
    "    (x_train, _), (x_test, _) = mnist.load_data()\n",
    "    x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.\n",
    "    x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.\n",
    "    return x_train, x_test\n",
    "\n",
    "def build_autoencoder():\n",
    "    \"\"\"Build and compile the CNN-based autoencoder.\"\"\"\n",
    "    # Input layer: accepts images of shape 28x28x1 (MNIST images)\n",
    "    input_img = Input(shape=(28, 28, 1))\n",
    "    \n",
    "    # Encoder\n",
    "    # Convolutional layer with 32 filters, each 3x3, using 'relu' activation. 'same' padding ensures output size matches input size.\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    # Max pooling layer to reduce spatial dimensions by half, improving computational efficiency and helping encode positional information.\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # Another convolutional layer with 16 filters to further extract features from the image.\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    # Reducing spatial dimensions again to further compress the representation.\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # Final convolutional layer in the encoder with 8 filters, focusing on the most abstract features of the image.\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    # Last max pooling layer in the encoder to achieve the final compressed representation.\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    # Convolutional layer with 8 filters, starting the process of decoding the compressed representation.\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    # Upsampling layer to start expanding the spatial dimensions back to the original size.\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    # Convolutional layer with 16 filters to further refine the decoded features.\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    # Upsampling again to get closer to the original image size.\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    # Convolutional layer with 32 filters, nearly restoring the original depth of features.\n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x)  # Note: No padding here, changes size slightly.\n",
    "    # Final upsampling to match the original image dimensions.\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    # Output layer to reconstruct the original image. Uses 'sigmoid' activation to output pixel values between 0 and 1.\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    # Compiling the autoencoder model with Adam optimizer and binary cross-entropy loss.\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, x_train, x_test):\n",
    "    \"\"\"Train the autoencoder.\"\"\"\n",
    "    autoencoder.fit(x_train, x_train, epochs=1, batch_size=128, shuffle=True, validation_data=(x_test, x_test))\n",
    "\n",
    "def generate_embeddings(encoder, x_test):\n",
    "    \"\"\"Generate embeddings for the test set.\"\"\"\n",
    "    return encoder.predict(x_test)\n",
    "\n",
    "def find_similar_images(artist_embeddings, copied_embeddings):\n",
    "    \"\"\"Find and return indices of similar images based on embeddings.\"\"\"\n",
    "    similar_images_indices = []\n",
    "    for embedding in copied_embeddings:\n",
    "        distances = cdist([embedding], artist_embeddings, 'euclidean')\n",
    "        closest_indices = np.argsort(distances)[0][0:4]  # Exclude self\n",
    "        similar_images_indices.append(closest_indices)\n",
    "    return similar_images_indices\n",
    "\n",
    "def display_similar_images(x_test, selected_indices, similar_images_indices):\n",
    "    \"\"\"Visualize the original and similar images.\"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for i, (index, sim_indices) in enumerate(zip(selected_indices, similar_images_indices)):\n",
    "        ax = plt.subplot(3, 4, i * 4 + 1)\n",
    "        plt.imshow(x_test[index].reshape(28, 28))\n",
    "        plt.title(\"Original\")\n",
    "        plt.gray()\n",
    "        ax.axis('off')\n",
    "        \n",
    "        for j, sim_index in enumerate(sim_indices):\n",
    "            ax = plt.subplot(3, 4, i * 4 + j + 2)\n",
    "            plt.imshow(x_test[sim_index].reshape(28, 28))\n",
    "            plt.title(f\"Similar {j+1}\")\n",
    "            plt.gray()\n",
    "            ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main workflow\n",
    "x_train, x_test = load_and_preprocess_data()\n",
    "autoencoder = build_autoencoder()\n",
    "train_autoencoder(autoencoder, x_train, x_test)\n",
    "\n",
    "encoder = Model(autoencoder.input, autoencoder.layers[-7].output)\n",
    "encoded_imgs = generate_embeddings(encoder, np.reshape(x_test, (len(x_test), 28, 28, 1)))\n",
    "encoded_imgs_flatten = encoded_imgs.reshape((len(x_test), np.prod(encoded_imgs.shape[1:])))\n",
    "\n",
    "np.random.seed(0)\n",
    "selected_indices = np.random.choice(x_test.shape[0], 3, replace=False)\n",
    "print(encoded_imgs_flatten[selected_indices])\n",
    "similar_images_indices = find_similar_images(encoded_imgs_flatten, encoded_imgs_flatten[selected_indices])\n",
    "display_similar_images(x_test, selected_indices, similar_images_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
